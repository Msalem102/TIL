{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9fqolVC6uA1f1frEEwFOQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Msalem102/TIL/blob/main/shallow_neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "9ACNTqKEOLWZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block of code attempts to manually implement the ReLU"
      ],
      "metadata": {
        "id": "04FLAJxVPpGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ReLU(preactivation):\n",
        "  return np.maximum(0, preactivation)"
      ],
      "metadata": {
        "id": "IR5lG8l9ObG3"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first construct the shallow neural network with one input, three hidden units, and one output"
      ],
      "metadata": {
        "id": "ClzuIJfGRUJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z = np.arange(-5,5,0.1)\n",
        "RelU_z = ReLU(z)\n",
        "\n",
        "# Plot the ReLU function\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(z,RelU_z,'r-')\n",
        "ax.set_xlim([-5,5]);ax.set_ylim([-5,5])\n",
        "ax.set_xlabel('z'); ax.set_ylabel('ReLU[z]')\n",
        "ax.set_aspect('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UT4HNhflPnLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a shallow neural network with, one input, one output, and three hidden units\n",
        "def shallow_1_1_3(x, activation_fn, phi_0,phi_1,phi_2,phi_3, theta_10, theta_11, theta_20, theta_21, theta_30, theta_31):\n",
        "  # from the theta parameters (i.e. implement equations at bottom of figure 3.3a-c).  These are the preactivations\n",
        "  pre_1 = theta_10 + np.dot(theta_11,x)\n",
        "  pre_2 = theta_20 + np.dot(theta_21,x)\n",
        "  pre_3 = theta_30 + np.dot(theta_31,x)\n",
        "\n",
        "  # Pass these through the ReLU function to compute the activations as in\n",
        "  # figure 3.3 d-f\n",
        "  act_1 = ReLU(pre_1)\n",
        "  act_2 = ReLU(pre_2)\n",
        "  act_3 = ReLU(pre_3)\n",
        "\n",
        "  # To create the equivalent of figure 3.3 g-i\n",
        "  w_act_1 = phi_1 * act_1\n",
        "  w_act_2 = phi_2 * act_2\n",
        "  w_act_3 = phi_3 * act_3\n",
        "\n",
        "  # phi_0 to create the output as in figure 3.3 j\n",
        "  y = phi_0 + w_act_1 + w_act_2 + w_act_3\n",
        "\n",
        "  # Return everything we have calculated\n",
        "  return y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3"
      ],
      "metadata": {
        "id": "9s3oWQZ8SGab"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the shallow neural network.  We'll assume input in is range [0,1] and output [-1,1]\n",
        "# If the plot_all flag is set to true, then we'll plot all the intermediate stages as in Figure 3.3\n",
        "def plot_neural(x, y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3, plot_all=False, x_data=None, y_data=None):\n",
        "\n",
        "  # Plot intermediate plots if flag set\n",
        "  if plot_all:\n",
        "    fig, ax = plt.subplots(3,3)\n",
        "    fig.set_size_inches(8.5, 8.5)\n",
        "    fig.tight_layout(pad=3.0)\n",
        "    ax[0,0].plot(x,pre_1,'r-'); ax[0,0].set_ylabel('Preactivation')\n",
        "    ax[0,1].plot(x,pre_2,'b-'); ax[0,1].set_ylabel('Preactivation')\n",
        "    ax[0,2].plot(x,pre_3,'g-'); ax[0,2].set_ylabel('Preactivation')\n",
        "    ax[1,0].plot(x,act_1,'r-'); ax[1,0].set_ylabel('Activation')\n",
        "    ax[1,1].plot(x,act_2,'b-'); ax[1,1].set_ylabel('Activation')\n",
        "    ax[1,2].plot(x,act_3,'g-'); ax[1,2].set_ylabel('Activation')\n",
        "    ax[2,0].plot(x,w_act_1,'r-'); ax[2,0].set_ylabel('Weighted Act')\n",
        "    ax[2,1].plot(x,w_act_2,'b-'); ax[2,1].set_ylabel('Weighted Act')\n",
        "    ax[2,2].plot(x,w_act_3,'g-'); ax[2,2].set_ylabel('Weighted Act')\n",
        "\n",
        "    for plot_y in range(3):\n",
        "      for plot_x in range(3):\n",
        "        ax[plot_y,plot_x].set_xlim([0,1]);ax[plot_x,plot_y].set_ylim([-1,1])\n",
        "        ax[plot_y,plot_x].set_aspect(0.5)\n",
        "      ax[2,plot_y].set_xlabel('Input, $x$');\n",
        "    plt.show()\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.plot(x,y)\n",
        "  ax.set_xlabel('Input, $x$'); ax.set_ylabel('Output, $y$')\n",
        "  ax.set_xlim([0,1]);ax.set_ylim([-1,1])\n",
        "  ax.set_aspect(0.5)\n",
        "  if x_data is not None:\n",
        "    ax.plot(x_data, y_data, 'mo')\n",
        "    for i in range(len(x_data)):\n",
        "      ax.plot(x_data[i], y_data[i],)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "Uot21QN3P3f3"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now lets define some parameters and run the neural network\n",
        "theta_10 =  0.3 ; theta_11 = -1.0\n",
        "theta_20 = -1.0  ; theta_21 = 2.0\n",
        "theta_30 = -0.5  ; theta_31 = 0.65\n",
        "phi_0 = -0.3; phi_1 = 1.0; phi_2 = -.5; phi_3 = 7.0\n",
        "\n",
        "# Define a range of input values\n",
        "x = np.arange(0,100,0.01)\n",
        "\n",
        "# We run the neural network for each of these input values\n",
        "y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3 = \\\n",
        "    shallow_1_1_3(x, ReLU, phi_0,phi_1,phi_2,phi_3, theta_10, theta_11, theta_20, theta_21, theta_30, theta_31)\n",
        "# And then plot it\n",
        "plot_neural(x, y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3, plot_all=True)"
      ],
      "metadata": {
        "id": "B6RUdzhNQo5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osonHsEqVp2I"
      },
      "source": [
        "# Least squares loss\n",
        "\n",
        "Now let's consider fitting the network to data.  First we need to define the loss function.  We'll use the least squares loss:\n",
        "\n",
        "\\begin{equation}\n",
        "L[\\boldsymbol\\phi] = \\sum_{i=1}^{I}(y_{i}-\\text{f}[x_{i},\\boldsymbol\\phi])^2\n",
        "\\end{equation}\n",
        "\n",
        "where $(x_i,y_i)$ is an input/output training pair and $\\text{f}[\\bullet,\\boldsymbol\\phi]$ is the neural network with parameters $\\boldsymbol\\phi$.  The first term in the brackets is the ground truth output and the second term is the prediction of the model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Least squares function\n",
        "def least_squares_loss(y_train, y_predict):\n",
        "  loss = np.sum((y_train - y_predict)**2)\n",
        "\n",
        "  return loss"
      ],
      "metadata": {
        "id": "nhpul3w0WxR4"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6GXjtRubZ2U"
      },
      "outputs": [],
      "source": [
        "# Now lets define some parameters, run the neural network, and compute the loss\n",
        "theta_10 =  0.3 ; theta_11 = -1.0\n",
        "theta_20 = -1.0  ; theta_21 = 2.0\n",
        "theta_30 = -0.5  ; theta_31 = 0.65\n",
        "phi_0 = -0.3; phi_1 = 2.0; phi_2 = -1.0; phi_3 = 7.0\n",
        "\n",
        "# Define a range of input values\n",
        "x = np.arange(0,1,0.01)\n",
        "\n",
        "x_train = np.array([0.09291784,0.46809093,0.93089486,0.67612654,0.73441752,0.86847339,\\\n",
        "                   0.49873225,0.51083168,0.18343972,0.99380898,0.27840809,0.38028817,\\\n",
        "                   0.12055708,0.56715537,0.92005746,0.77072270,0.85278176,0.05315950,\\\n",
        "                   0.87168699,0.58858043])\n",
        "y_train = np.array([-0.15934537,0.18195445,0.451270150,0.13921448,0.09366691,0.30567674,\\\n",
        "                    0.372291170,0.40716968,-0.08131792,0.41187806,0.36943738,0.3994327,\\\n",
        "                    0.019062570,0.35820410,0.452564960,-0.0183121,0.02957665,-0.24354444, \\\n",
        "                    0.148038840,0.26824970])\n",
        "\n",
        "# We run the neural network for each of these input values\n",
        "y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3 = \\\n",
        "    shallow_1_1_3(x, ReLU, phi_0,phi_1,phi_2,phi_3, theta_10, theta_11, theta_20, theta_21, theta_30, theta_31)\n",
        "# And then plot it\n",
        "plot_neural(x, y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3, plot_all=True, x_data = x_train, y_data = y_train)\n",
        "\n",
        "# Run the neural network on the training data\n",
        "y_predict, *_ = shallow_1_1_3(x_train, ReLU, phi_0,phi_1,phi_2,phi_3, theta_10, theta_11, theta_20, theta_21, theta_30, theta_31)\n",
        "\n",
        "# Compute the least squares loss and print it out\n",
        "loss = least_squares_loss(y_train,y_predict)\n",
        "print('Your Loss = %3.3f, True value = 9.385'%(loss))\n",
        "\n"
      ]
    }
  ]
}